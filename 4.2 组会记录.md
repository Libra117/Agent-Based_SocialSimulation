很多个讨论的点
## agent和LLM的区分
agent定义到底是什么？

> 各个论文中给出的定义：
> 具有提示规范（初始状态）、对话跟踪（状态）和与工具使用（动作）等环境交互能力的人工实体——感觉是强化学习的agent逻辑，与LLM做了一些融合“Why Do Multi-Agent LLM Systems Fail?” (Cemri 等, 2025, p. 1) 

可能单独的agent比LLM的区别：
* 使用了额外的module，比如记忆模块，使用RAG处理大量信息
* ==更好的处理上下文==
* agent = LLM + context，LLM核心大脑，需要维护context信息
* agent内部实际的planning和LLM差不多，LLM多步执行、定义的workflow
agent可能可以分为两种：
1. 助手，tool, planning的过程，可能一些LLM已经有了这样的能力，如deepseek
2. role play，模拟，不同agent有不同的scale
多agent好处：不同角色、记忆的交互，比如debate场景
* 更好的处理信息，类似于强行进行内存访问优化
* 任务本身进行了拆分
## 多智能体任务选取
如何找到复杂的任务，这个工作涉及到的任务并不复杂
* 任务本身，有的任务本身可能就并不适合多智能体协作
* social simulation任务本身要求Agent规模较大、本身就需要多智能体设计
## 为什么要有向无环图
可以直接利用拓扑排序进行拆分，得到一个交互的顺序

## 整体总结
1. 到底什么任务适合多智能体协作
	1. 单智能体就能做好的任务是否仍然需要多智能体
2. 这个架构只是一种设计，并不能够代表多智能体
多智能体到底是否有必要？
* 上下文长度变短带来的增益
* 发现辩论场景下，LLM难以站在两个视角下给出正反方观点，但agent可以，因为agent有显式memory设计，但是LLM只是context不一样
* 所以区别在于memory，而后带来上下文有区别
多智能体什么场景下有必要？
* 什么应该是定义好的，什么是可以学习、可以设计的地方
* 结构可以学习是否有意义？应该是有意义的但是需要学得好
	* 这个模型可以看出不同interaction差异很大——可学习的空间也很大
* 比如用比较小的场景去模拟人，看看能否会产生类似的社会现象
什么是比较难的任务
* 这个工作做的是通用的任务，反向的去验证组织架构比单独agent好、有什么共性
* 也可以是specific任务
现在这个框架更像是一个评价的过程、并没有优化的过程
* 定义好的框架、交互，然后看模型的结果如何
最终愿景：
* 希望能够用agent合作解决bias、幻觉
* 后续可能我们问LLM，实际输出的事几个agent在低层交互之后的结果
